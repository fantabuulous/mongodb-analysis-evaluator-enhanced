"""
ë²”ìš© MongoDB ë¶„ì„ ìë™ í‰ê°€ ì‹œìŠ¤í…œ
Evidently ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ 4ê°œ í•µì‹¬ ì§€í‘œ ìë™ ì‚°ì¶œ ë° Pass/Fail íŒì •

ì°¸ê³ í•œ Evidently ì½”ë“œ:
1. evidently.test_suite.TestSuite - ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í”„ë ˆì„ì›Œí¬
2. evidently.tests.base_test.Test - ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ êµ¬í˜„ì„ ìœ„í•œ ë² ì´ìŠ¤ í´ë˜ìŠ¤
3. evidently.metrics.base_metric.Metric - ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ êµ¬í˜„ì„ ìœ„í•œ ë² ì´ìŠ¤ í´ë˜ìŠ¤
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass
from datetime import datetime
import json
import traceback
import re


@dataclass
class UniversalAnalysisResult:
    """ë²”ìš© ë¶„ì„ ê²°ê³¼ êµ¬ì¡°"""
    analysis_query: str                    # ë¶„ì„ ì§ˆì˜
    mongodb_queries: List[str]             # ì‹¤í–‰ëœ MongoDB ì¿¼ë¦¬ë“¤
    calculation_results: Dict[str, Any]    # ê³„ì‚° ê²°ê³¼ (LLM ì‚°ì¶œ)
    execution_logs: List[Dict]             # ì‹¤í–‰ ë¡œê·¸
    direct_mongodb_results: Optional[Dict[str, Any]] = None  # MongoDB ì§ì ‘ ì‹¤í–‰ ê²°ê³¼
    timestamp: str = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()


@dataclass
class EvaluationMetrics:
    """4ê°œ í•µì‹¬ í‰ê°€ ì§€í‘œ"""
    semantic_error_rate: float      # ì˜ë¯¸ ì˜¤ë¥˜ ë¹„ìœ¨
    execution_success_rate: float   # ì‹¤í–‰ ì„±ê³µë¥   
    empty_result_rate: float        # ë¬´ì‘ë‹µë¥ 
    accuracy_rate: float            # ì •ë‹µ ì¼ì¹˜ìœ¨
    overall_pass: bool              # ì „ì²´ Pass/Fail
    comparison_table: Optional[pd.DataFrame] = None  # ë¹„êµ í…Œì´ë¸”


@dataclass
class QueryComparisonResult:
    """ì¿¼ë¦¬ ë¹„êµ ê²°ê³¼"""
    query: str
    llm_result: Any
    mongodb_result: Any
    match: bool
    difference: Optional[str] = None


class UniversalMongoDBEvaluator:
    """
    ë²”ìš© MongoDB ë¶„ì„ í‰ê°€ê¸°
    ëª¨ë“  ì¢…ë¥˜ì˜ MongoDB ë¶„ì„ì— ì ìš© ê°€ëŠ¥í•œ ë²”ìš© í‰ê°€ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, thresholds: Optional[Dict[str, float]] = None):
        """
        Args:
            thresholds: í‰ê°€ ì„ê³„ê°’ ì„¤ì •
                - semantic_error: ì˜ë¯¸ ì˜¤ë¥˜ìœ¨ ì„ê³„ê°’ (ê¸°ë³¸ 0.1)
                - execution_success: ì‹¤í–‰ ì„±ê³µë¥  ì„ê³„ê°’ (ê¸°ë³¸ 0.8)
                - empty_result: ë¬´ì‘ë‹µë¥  ì„ê³„ê°’ (ê¸°ë³¸ 0.2)
                - accuracy: ì •ë‹µ ì¼ì¹˜ìœ¨ ì„ê³„ê°’ (ê¸°ë³¸ 0.9)
        """
        self.thresholds = thresholds or {
            "semantic_error": 0.1,
            "execution_success": 0.8,
            "empty_result": 0.2,
            "accuracy": 0.9
        }
    
    def evaluate(self, analysis_result: UniversalAnalysisResult, 
                ground_truth: Optional[Dict[str, Any]] = None) -> EvaluationMetrics:
        """
        ë¶„ì„ ê²°ê³¼ í‰ê°€ ì‹¤í–‰
        
        Args:
            analysis_result: ë¶„ì„ ê²°ê³¼
            ground_truth: ì •ë‹µ ë°ì´í„° (ì„ íƒì )
            
        Returns:
            EvaluationMetrics: 4ê°œ í•µì‹¬ ì§€í‘œ ë° Pass/Fail ê²°ê³¼
        """
        
        # ë¹„êµ í…Œì´ë¸” ìƒì„± (LLM vs MongoDB ì§ì ‘ ì‹¤í–‰)
        comparison_table = self._create_comparison_table(analysis_result)
        
        # 1. ì˜ë¯¸ ì˜¤ë¥˜ìœ¨ ê³„ì‚°
        semantic_error_rate = self._calculate_semantic_error_rate(analysis_result)
        
        # 2. ì‹¤í–‰ ì„±ê³µë¥  ê³„ì‚°
        execution_success_rate = self._calculate_execution_success_rate(analysis_result)
        
        # 3. ë¬´ì‘ë‹µë¥  ê³„ì‚°
        empty_result_rate = self._calculate_empty_result_rate(analysis_result)
        
        # 4. ì •ë‹µ ì¼ì¹˜ìœ¨ ê³„ì‚°
        accuracy_rate = self._calculate_accuracy_rate(analysis_result, ground_truth)
        
        # 5. ì „ì²´ Pass/Fail íŒì •
        overall_pass = self._determine_overall_pass(
            semantic_error_rate, execution_success_rate, 
            empty_result_rate, accuracy_rate
        )
        
        return EvaluationMetrics(
            semantic_error_rate=semantic_error_rate,
            execution_success_rate=execution_success_rate,
            empty_result_rate=empty_result_rate,
            accuracy_rate=accuracy_rate,
            overall_pass=overall_pass,
            comparison_table=comparison_table
        )
    
    def _create_comparison_table(self, analysis_result: UniversalAnalysisResult) -> pd.DataFrame:
        """LLM ê³„ì‚° ê²°ê³¼ì™€ MongoDB ì§ì ‘ ì‹¤í–‰ ê²°ê³¼ ë¹„êµ í…Œì´ë¸” ìƒì„±"""
        
        comparison_data = []
        
        # LLM ê³„ì‚° ê²°ê³¼ì™€ MongoDB ì§ì ‘ ì‹¤í–‰ ê²°ê³¼ ë¹„êµ
        for metric_name, llm_value in analysis_result.calculation_results.items():
            mongodb_value = None
            match_status = "N/A"
            difference = ""
            
            # MongoDB ì§ì ‘ ì‹¤í–‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
            if (analysis_result.direct_mongodb_results and 
                metric_name in analysis_result.direct_mongodb_results):
                
                mongodb_value = analysis_result.direct_mongodb_results[metric_name]
                
                # ê°’ ë¹„êµ
                if self._values_match(llm_value, mongodb_value):
                    match_status = "âœ… ì¼ì¹˜"
                    difference = "0"
                else:
                    match_status = "âŒ ë¶ˆì¼ì¹˜"
                    difference = self._calculate_difference(llm_value, mongodb_value)
            
            comparison_data.append({
                "ì§€í‘œ": metric_name,
                "LLM ê³„ì‚° ê²°ê³¼": self._format_value(llm_value),
                "MongoDB ì§ì ‘ ì‹¤í–‰": self._format_value(mongodb_value),
                "ì¼ì¹˜ ì—¬ë¶€": match_status,
                "ì°¨ì´": difference
            })
        
        # MongoDBì—ë§Œ ìˆëŠ” ê²°ê³¼ ì¶”ê°€
        if analysis_result.direct_mongodb_results:
            for metric_name, mongodb_value in analysis_result.direct_mongodb_results.items():
                if metric_name not in analysis_result.calculation_results:
                    comparison_data.append({
                        "ì§€í‘œ": metric_name,
                        "LLM ê³„ì‚° ê²°ê³¼": "N/A",
                        "MongoDB ì§ì ‘ ì‹¤í–‰": self._format_value(mongodb_value),
                        "ì¼ì¹˜ ì—¬ë¶€": "âš ï¸ LLM ëˆ„ë½",
                        "ì°¨ì´": "N/A"
                    })
        
        return pd.DataFrame(comparison_data)
    
    def _format_value(self, value: Any) -> str:
        """ê°’ì„ í‘œì‹œìš©ìœ¼ë¡œ í¬ë§·íŒ…"""
        if value is None:
            return "N/A"
        elif isinstance(value, float):
            if value.is_integer():
                return str(int(value))
            else:
                return f"{value:.2f}"
        elif isinstance(value, (list, dict)):
            return f"{type(value).__name__}({len(value)})"
        else:
            return str(value)
    
    def _calculate_difference(self, llm_value: Any, mongodb_value: Any) -> str:
        """ë‘ ê°’ì˜ ì°¨ì´ ê³„ì‚°"""
        try:
            if isinstance(llm_value, (int, float)) and isinstance(mongodb_value, (int, float)):
                diff = abs(llm_value - mongodb_value)
                if mongodb_value != 0:
                    percentage = (diff / abs(mongodb_value)) * 100
                    return f"{diff:.2f} ({percentage:.1f}%)"
                else:
                    return f"{diff:.2f}"
            else:
                return "íƒ€ì… ë¶ˆì¼ì¹˜"
        except:
            return "ê³„ì‚° ë¶ˆê°€"
    
    def _calculate_semantic_error_rate(self, analysis_result: UniversalAnalysisResult) -> float:
        """ì˜ë¯¸ ì˜¤ë¥˜ìœ¨ ê³„ì‚°"""
        if not analysis_result.mongodb_queries:
            return 0.0
        
        semantic_errors = 0
        total_queries = len(analysis_result.mongodb_queries)
        
        for query in analysis_result.mongodb_queries:
            if self._has_semantic_error(query, analysis_result):
                semantic_errors += 1
        
        return semantic_errors / total_queries if total_queries > 0 else 0.0
    
    def _has_semantic_error(self, query: str, analysis_result: UniversalAnalysisResult) -> bool:
        """ê°œë³„ ì¿¼ë¦¬ì˜ ì˜ë¯¸ ì˜¤ë¥˜ ê²€ì‚¬"""
        
        # ë…¼ë¦¬ì  ëª¨ìˆœ íŒ¨í„´ ê²€ì‚¬
        logical_error_patterns = [
            r"(\w+)\s*[!=<>]+\s*\1",           # ë™ì¼ í•„ë“œ ë¹„êµ (field != field)
            r"count.*==.*0.*AND.*exists",      # ì¡´ì¬í•˜ë©´ì„œ ê°œìˆ˜ê°€ 0ì¸ ëª¨ìˆœ
            r">\s*9{8,}",                      # ë¹„í˜„ì‹¤ì ì¸ í° ìˆ˜
            r"\$match.*\{\s*\}",               # ë¹ˆ ë§¤ì¹˜ ì¡°ê±´
            r"\$group.*_id.*_id",              # ì¤‘ë³µ ê·¸ë£¹í•‘
        ]
        
        for pattern in logical_error_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                return True
        
        # ì‹¤í–‰ ê²°ê³¼ì™€ ì¿¼ë¦¬ ì˜ë„ ë¶ˆì¼ì¹˜ ê²€ì‚¬
        if self._check_result_query_mismatch(query, analysis_result):
            return True
        
        return False
    
    def _check_result_query_mismatch(self, query: str, analysis_result: UniversalAnalysisResult) -> bool:
        """ì¿¼ë¦¬ ì˜ë„ì™€ ê²°ê³¼ ë¶ˆì¼ì¹˜ ê²€ì‚¬"""
        
        # count ì¿¼ë¦¬ì¸ë° ê²°ê³¼ê°€ ìŒìˆ˜
        if "count" in query.lower():
            for key, value in analysis_result.calculation_results.items():
                if isinstance(value, (int, float)) and value < 0:
                    return True
        
        # ë¹„ìœ¨ ê³„ì‚°ì¸ë° 100% ì´ˆê³¼
        if any(word in analysis_result.analysis_query.lower() 
               for word in ["ë¹„ìœ¨", "ìœ¨", "percent", "rate"]):
            for key, value in analysis_result.calculation_results.items():
                if isinstance(value, (int, float)) and value > 100:
                    return True
        
        return False
    
    def _calculate_execution_success_rate(self, analysis_result: UniversalAnalysisResult) -> float:
        """ì‹¤í–‰ ì„±ê³µë¥  ê³„ì‚°"""
        if not analysis_result.execution_logs:
            return 1.0  # ë¡œê·¸ê°€ ì—†ìœ¼ë©´ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        
        successful_executions = 0
        total_executions = len(analysis_result.execution_logs)
        
        for log in analysis_result.execution_logs:
            # ì‹¤í–‰ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            if log.get("status") == "success" or log.get("error") is None:
                successful_executions += 1
        
        return successful_executions / total_executions if total_executions > 0 else 1.0
    
    def _calculate_empty_result_rate(self, analysis_result: UniversalAnalysisResult) -> float:
        """ë¬´ì‘ë‹µë¥  ê³„ì‚°"""
        if not analysis_result.calculation_results:
            return 1.0  # ê²°ê³¼ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ 100% ë¬´ì‘ë‹µ
        
        empty_results = 0
        total_results = len(analysis_result.calculation_results)
        
        for key, value in analysis_result.calculation_results.items():
            if self._is_empty_or_invalid_result(value):
                empty_results += 1
        
        return empty_results / total_results if total_results > 0 else 1.0
    
    def _is_empty_or_invalid_result(self, value: Any) -> bool:
        """ë¹ˆ ê²°ê³¼ ë˜ëŠ” ë¬´íš¨ ê²°ê³¼ ê²€ì‚¬"""
        
        # None ë˜ëŠ” ë¹ˆ ê°’
        if value is None:
            return True
        
        # ë¹ˆ ì»¬ë ‰ì…˜
        if isinstance(value, (list, dict)) and len(value) == 0:
            return True
        
        # ë¹ˆ ë¬¸ìì—´
        if isinstance(value, str) and value.strip() == "":
            return True
        
        # NaN ë˜ëŠ” ë¬´í•œëŒ€
        if isinstance(value, (int, float)):
            if np.isnan(value) or np.isinf(value):
                return True
        
        # ì—ëŸ¬ ë©”ì‹œì§€ê°€ í¬í•¨ëœ ê²°ê³¼
        if isinstance(value, str) and any(word in value.lower() 
                                         for word in ["error", "exception", "failed", "null"]):
            return True
        
        return False
    
    def _calculate_accuracy_rate(self, analysis_result: UniversalAnalysisResult, 
                                ground_truth: Optional[Dict[str, Any]]) -> float:
        """ì •ë‹µ ì¼ì¹˜ìœ¨ ê³„ì‚°"""
        
        # MongoDB ì§ì ‘ ì‹¤í–‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ìš°ì„  ì‚¬ìš©
        if analysis_result.direct_mongodb_results:
            return self._calculate_mongodb_comparison_accuracy(analysis_result)
        
        if ground_truth is None:
            # Ground truthê°€ ì—†ìœ¼ë©´ ì¼ê´€ì„± ê¸°ë°˜ ì •í™•ë„ ê³„ì‚°
            return self._calculate_consistency_score(analysis_result)
        
        correct_results = 0
        total_comparisons = 0
        
        for key, calculated_value in analysis_result.calculation_results.items():
            if key in ground_truth:
                expected_value = ground_truth[key]
                
                if self._values_match(calculated_value, expected_value):
                    correct_results += 1
                total_comparisons += 1
        
        return correct_results / total_comparisons if total_comparisons > 0 else 1.0
    
    def _calculate_mongodb_comparison_accuracy(self, analysis_result: UniversalAnalysisResult) -> float:
        """MongoDB ì§ì ‘ ì‹¤í–‰ ê²°ê³¼ì™€ì˜ ë¹„êµë¥¼ í†µí•œ ì •í™•ë„ ê³„ì‚°"""
        correct_results = 0
        total_comparisons = 0
        
        for key, llm_value in analysis_result.calculation_results.items():
            if key in analysis_result.direct_mongodb_results:
                mongodb_value = analysis_result.direct_mongodb_results[key]
                
                if self._values_match(llm_value, mongodb_value):
                    correct_results += 1
                total_comparisons += 1
        
        return correct_results / total_comparisons if total_comparisons > 0 else 0.0
    
    def _values_match(self, calculated: Any, expected: Any, tolerance: float = 0.01) -> bool:
        """ë‘ ê°’ì˜ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸ (í—ˆìš© ì˜¤ì°¨ í¬í•¨)"""
        
        # íƒ€ì…ì´ ë‹¤ë¥´ë©´ ë¶ˆì¼ì¹˜
        if type(calculated) != type(expected):
            # ìˆ«ì íƒ€ì… ê°„ ë¹„êµëŠ” í—ˆìš©
            if not (isinstance(calculated, (int, float)) and isinstance(expected, (int, float))):
                return False
        
        # ìˆ«ì ë¹„êµ (í—ˆìš© ì˜¤ì°¨ í¬í•¨)
        if isinstance(calculated, (int, float)) and isinstance(expected, (int, float)):
            return abs(calculated - expected) <= tolerance
        
        # ë¬¸ìì—´ ë¹„êµ
        if isinstance(calculated, str) and isinstance(expected, str):
            return calculated.strip().lower() == expected.strip().lower()
        
        # ë¦¬ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ë¹„êµ
        if isinstance(calculated, (list, dict)) and isinstance(expected, (list, dict)):
            return calculated == expected
        
        # ì¼ë°˜ì ì¸ ë™ë“±ì„± ë¹„êµ
        return calculated == expected
    
    def _calculate_consistency_score(self, analysis_result: UniversalAnalysisResult) -> float:
        """Ground truthê°€ ì—†ì„ ë•Œ ì¼ê´€ì„± ê¸°ë°˜ ì •í™•ë„ ê³„ì‚°"""
        
        # ê²°ê³¼ ê°’ë“¤ì˜ ì¼ê´€ì„± ê²€ì‚¬
        consistency_score = 1.0
        
        # ìˆ˜ì¹˜ ê²°ê³¼ì˜ í•©ë¦¬ì„± ê²€ì‚¬
        numeric_results = [v for v in analysis_result.calculation_results.values() 
                          if isinstance(v, (int, float))]
        
        if numeric_results:
            # ê·¹ê°’ ê²€ì‚¬ (ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì€ ê°’)
            for value in numeric_results:
                if value < 0 and "count" in str(analysis_result.calculation_results):
                    consistency_score -= 0.2  # ê°œìˆ˜ê°€ ìŒìˆ˜ë©´ ê°ì 
                
                if value > 10000000:  # ì²œë§Œ ì´ìƒì˜ í° ê°’
                    consistency_score -= 0.1
        
        # ë¹„ìœ¨ ê²°ê³¼ì˜ í•©ë¦¬ì„± ê²€ì‚¬ (0-100% ë²”ìœ„)
        for key, value in analysis_result.calculation_results.items():
            if any(word in key.lower() for word in ["rate", "ratio", "ë¹„ìœ¨", "ìœ¨"]):
                if isinstance(value, (int, float)) and (value < 0 or value > 100):
                    consistency_score -= 0.3
        
        return max(0.0, consistency_score)
    
    def _determine_overall_pass(self, semantic_error_rate: float, 
                               execution_success_rate: float,
                               empty_result_rate: float, 
                               accuracy_rate: float) -> bool:
        """ì „ì²´ Pass/Fail íŒì •"""
        
        # ê° ì§€í‘œê°€ ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸
        semantic_pass = semantic_error_rate <= self.thresholds["semantic_error"]
        execution_pass = execution_success_rate >= self.thresholds["execution_success"]
        empty_pass = empty_result_rate <= self.thresholds["empty_result"]
        accuracy_pass = accuracy_rate >= self.thresholds["accuracy"]
        
        # ëª¨ë“  ì§€í‘œê°€ í†µê³¼í•´ì•¼ ì „ì²´ í†µê³¼
        return semantic_pass and execution_pass and empty_pass and accuracy_pass
    
    def generate_comprehensive_report(self, metrics: EvaluationMetrics, 
                                    analysis_result: UniversalAnalysisResult) -> str:
        """í¬ê´„ì ì¸ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± (ë¹„êµ í…Œì´ë¸” í¬í•¨)"""
        
        status_emoji = "âœ… PASS" if metrics.overall_pass else "âŒ FAIL"
        
        report = f"""
# MongoDB ë¶„ì„ í‰ê°€ ê²°ê³¼

## ì „ì²´ ê²°ê³¼: {status_emoji}

### ë¶„ì„ ì§ˆì˜
```
{analysis_result.analysis_query}
```

### LLM vs MongoDB ì§ì ‘ ì‹¤í–‰ ë¹„êµ
"""
        
        # ë¹„êµ í…Œì´ë¸”ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if metrics.comparison_table is not None and not metrics.comparison_table.empty:
            report += "\n" + metrics.comparison_table.to_string(index=False) + "\n\n"
        else:
            report += "ë¹„êµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
        
        report += f"""
### 4ê°œ í•µì‹¬ ì§€í‘œ
| ì§€í‘œ | ê°’ | ì„ê³„ê°’ | ìƒíƒœ |
|------|----|----|------|
| ì˜ë¯¸ ì˜¤ë¥˜ìœ¨ | {metrics.semantic_error_rate:.2%} | â‰¤{self.thresholds['semantic_error']:.2%} | {'âœ…' if metrics.semantic_error_rate <= self.thresholds['semantic_error'] else 'âŒ'} |
| ì‹¤í–‰ ì„±ê³µë¥  | {metrics.execution_success_rate:.2%} | â‰¥{self.thresholds['execution_success']:.2%} | {'âœ…' if metrics.execution_success_rate >= self.thresholds['execution_success'] else 'âŒ'} |
| ë¬´ì‘ë‹µë¥  | {metrics.empty_result_rate:.2%} | â‰¤{self.thresholds['empty_result']:.2%} | {'âœ…' if metrics.empty_result_rate <= self.thresholds['empty_result'] else 'âŒ'} |
| ì •ë‹µ ì¼ì¹˜ìœ¨ | {metrics.accuracy_rate:.2%} | â‰¥{self.thresholds['accuracy']:.2%} | {'âœ…' if metrics.accuracy_rate >= self.thresholds['accuracy'] else 'âŒ'} |

### ì‹¤í–‰ëœ MongoDB ì¿¼ë¦¬ë“¤
"""
        
        for i, query in enumerate(analysis_result.mongodb_queries, 1):
            report += f"{i}. ```javascript\n{query}\n```\n\n"
        
        report += f"**í‰ê°€ ì‹œê°„**: {analysis_result.timestamp}\n"
        
        return report

    def generate_simple_report(self, metrics: EvaluationMetrics, 
                              analysis_result: UniversalAnalysisResult) -> str:
        """ê°„ë‹¨í•œ í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± (ê¸°ì¡´ ë²„ì „ ìœ ì§€)"""
        
        status_emoji = "âœ… PASS" if metrics.overall_pass else "âŒ FAIL"
        
        report = f"""
# MongoDB ë¶„ì„ í‰ê°€ ê²°ê³¼

## ì „ì²´ ê²°ê³¼: {status_emoji}

### ë¶„ì„ ì§ˆì˜
```
{analysis_result.analysis_query}
```

### 4ê°œ í•µì‹¬ ì§€í‘œ
| ì§€í‘œ | ê°’ | ì„ê³„ê°’ | ìƒíƒœ |
|------|----|----|------|
| ì˜ë¯¸ ì˜¤ë¥˜ìœ¨ | {metrics.semantic_error_rate:.2%} | â‰¤{self.thresholds['semantic_error']:.2%} | {'âœ…' if metrics.semantic_error_rate <= self.thresholds['semantic_error'] else 'âŒ'} |
| ì‹¤í–‰ ì„±ê³µë¥  | {metrics.execution_success_rate:.2%} | â‰¥{self.thresholds['execution_success']:.2%} | {'âœ…' if metrics.execution_success_rate >= self.thresholds['execution_success'] else 'âŒ'} |
| ë¬´ì‘ë‹µë¥  | {metrics.empty_result_rate:.2%} | â‰¤{self.thresholds['empty_result']:.2%} | {'âœ…' if metrics.empty_result_rate <= self.thresholds['empty_result'] else 'âŒ'} |
| ì •ë‹µ ì¼ì¹˜ìœ¨ | {metrics.accuracy_rate:.2%} | â‰¥{self.thresholds['accuracy']:.2%} | {'âœ…' if metrics.accuracy_rate >= self.thresholds['accuracy'] else 'âŒ'} |

### ê³„ì‚° ê²°ê³¼
"""
        
        for key, value in analysis_result.calculation_results.items():
            report += f"- **{key}**: {value}\n"
        
        report += f"\n**í‰ê°€ ì‹œê°„**: {analysis_result.timestamp}\n"
        
        return report


# ê°„í¸ ì‚¬ìš©ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
def quick_evaluate(analysis_query: str, 
                  mongodb_queries: List[str],
                  calculation_results: Dict[str, Any],
                  execution_logs: List[Dict] = None,
                  direct_mongodb_results: Dict[str, Any] = None,
                  ground_truth: Dict[str, Any] = None,
                  custom_thresholds: Dict[str, float] = None) -> EvaluationMetrics:
    """
    ë¹ ë¥¸ í‰ê°€ ì‹¤í–‰ì„ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
    
    Args:
        analysis_query: ë¶„ì„ ì§ˆì˜
        mongodb_queries: ì‹¤í–‰ëœ MongoDB ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
        calculation_results: ê³„ì‚° ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (LLM ì‚°ì¶œ)
        execution_logs: ì‹¤í–‰ ë¡œê·¸ (ì„ íƒì )
        direct_mongodb_results: MongoDB ì§ì ‘ ì‹¤í–‰ ê²°ê³¼ (ì„ íƒì )
        ground_truth: ì •ë‹µ ë°ì´í„° (ì„ íƒì )
        custom_thresholds: ì»¤ìŠ¤í…€ ì„ê³„ê°’ (ì„ íƒì )
    
    Returns:
        EvaluationMetrics: í‰ê°€ ê²°ê³¼
    """
    
    # ë¶„ì„ ê²°ê³¼ ê°ì²´ ìƒì„±
    analysis_result = UniversalAnalysisResult(
        analysis_query=analysis_query,
        mongodb_queries=mongodb_queries,
        calculation_results=calculation_results,
        execution_logs=execution_logs or [],
        direct_mongodb_results=direct_mongodb_results
    )
    
    # í‰ê°€ê¸° ìƒì„± ë° ì‹¤í–‰
    evaluator = UniversalMongoDBEvaluator(thresholds=custom_thresholds)
    metrics = evaluator.evaluate(analysis_result, ground_truth)
    
    return metrics


# ì‚¬ìš© ì˜ˆì œ
def example_usage():
    """í™•ì¥ëœ ì‚¬ìš© ì˜ˆì œ (ë¹„êµ í…Œì´ë¸” í¬í•¨)"""
    
    # 1. ë¶„ì„ ê²°ê³¼ ì¤€ë¹„
    analysis_query = "care_313 ì»¬ë ‰ì…˜ì—ì„œ ì‚¬ìš©ì ìˆ˜, ì±— ìˆ˜, ìš´ì˜ì ì—°ê²° ë¹„ìœ¨ì„ ë¶„ì„í•´ì¤˜"
    
    mongodb_queries = [
        "db.care_313.find({'message': 'Received retrieval request'})",
        "db.care_313.aggregate([{'$group': {'_id': '$user_id'}}])",
        "db.care_313.find({'content': {'$regex': 'ìš´ì˜ì ì—°ê²°'}})"
    ]
    
    # LLMì´ ê³„ì‚°í•œ ê²°ê³¼
    calculation_results = {
        "user_count": 3,
        "chat_count": 8,
        "operator_connection_rate": 33.33
    }
    
    # MongoDBì—ì„œ ì§ì ‘ ì‹¤í–‰í•œ ê²°ê³¼ (ì‹¤ì œ ê°’)
    direct_mongodb_results = {
        "user_count": 3,      # ì¼ì¹˜
        "chat_count": 7,      # ë¶ˆì¼ì¹˜ (LLM: 8, ì‹¤ì œ: 7)
        "operator_connection_rate": 35.71  # ë¶ˆì¼ì¹˜ (LLM: 33.33%, ì‹¤ì œ: 35.71%)
    }
    
    execution_logs = [
        {"status": "success", "query_index": 0, "execution_time": 0.1},
        {"status": "success", "query_index": 1, "execution_time": 0.2},
        {"status": "success", "query_index": 2, "execution_time": 0.1}
    ]
    
    # 2. ë¹ ë¥¸ í‰ê°€ ì‹¤í–‰ (ë¹„êµ ë°ì´í„° í¬í•¨)
    metrics = quick_evaluate(
        analysis_query=analysis_query,
        mongodb_queries=mongodb_queries,
        calculation_results=calculation_results,
        execution_logs=execution_logs,
        direct_mongodb_results=direct_mongodb_results
    )
    
    # 3. ê²°ê³¼ í™•ì¸
    print(f"ì „ì²´ ê²°ê³¼: {'PASS' if metrics.overall_pass else 'FAIL'}")
    print(f"ì˜ë¯¸ ì˜¤ë¥˜ìœ¨: {metrics.semantic_error_rate:.2%}")
    print(f"ì‹¤í–‰ ì„±ê³µë¥ : {metrics.execution_success_rate:.2%}")
    print(f"ë¬´ì‘ë‹µë¥ : {metrics.empty_result_rate:.2%}")
    print(f"ì •ë‹µ ì¼ì¹˜ìœ¨: {metrics.accuracy_rate:.2%}")
    
    # 4. ë¹„êµ í…Œì´ë¸” ì¶œë ¥
    if metrics.comparison_table is not None:
        print("\n=== LLM vs MongoDB ì§ì ‘ ì‹¤í–‰ ë¹„êµ ===")
        print(metrics.comparison_table.to_string(index=False))
    
    # 5. í¬ê´„ì ì¸ ë¦¬í¬íŠ¸ ìƒì„±
    analysis_result = UniversalAnalysisResult(
        analysis_query=analysis_query,
        mongodb_queries=mongodb_queries,
        calculation_results=calculation_results,
        execution_logs=execution_logs,
        direct_mongodb_results=direct_mongodb_results
    )
    
    evaluator = UniversalMongoDBEvaluator()
    comprehensive_report = evaluator.generate_comprehensive_report(metrics, analysis_result)
    print("\n" + "="*60)
    print("COMPREHENSIVE REPORT")
    print("="*60)
    print(comprehensive_report)
    
    return metrics


def example_with_mcp_integration():
    """MCP(everything) ê³„ì‚°ê¸° ì—°ë™ ì˜ˆì œ"""
    
    # ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: LLMì´ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³  MCPë¡œ ê³„ì‚°, MongoDBì—ì„œ ì§ì ‘ ì‹¤í–‰í•˜ì—¬ ë¹„êµ
    
    analysis_query = "ì‚¬ìš©ìë³„ í‰ê·  ì„¸ì…˜ ì‹œê°„ê³¼ ì´ ì ‘ì† íšŸìˆ˜ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
    
    # LLMì´ ìƒì„±í•œ MongoDB ì¿¼ë¦¬ë“¤
    mongodb_queries = [
        """db.sessions.aggregate([
            {$group: {
                _id: "$user_id",
                avg_session_time: {$avg: "$duration"},
                total_sessions: {$sum: 1}
            }}
        ])""",
        """db.sessions.aggregate([
            {$group: {
                _id: null,
                overall_avg_time: {$avg: "$duration"},
                total_users: {$addToSet: "$user_id"}
            }},
            {$project: {
                overall_avg_time: 1,
                user_count: {$size: "$total_users"}
            }}
        ])"""
    ]
    
    # LLM + MCP(everything)ë¡œ ê³„ì‚°í•œ ê²°ê³¼
    llm_mcp_results = {
        "avg_session_time": 25.4,  # ë¶„
        "total_sessions": 156,
        "unique_users": 42,
        "avg_sessions_per_user": 3.71
    }
    
    # MongoDBì—ì„œ ì§ì ‘ ì‹¤í–‰í•œ ì‹¤ì œ ê²°ê³¼
    mongodb_direct_results = {
        "avg_session_time": 24.8,  #
        "avg_session_time": 24.8,  # ì‹¤ì œ ê°’ (LLMë³´ë‹¤ 0.6ë¶„ ë‚®ìŒ)
        "total_sessions": 158,     # ì‹¤ì œ ê°’ (LLMë³´ë‹¤ 2ê°œ ë§ìŒ) 
        "unique_users": 42,        # ì •í™•íˆ ì¼ì¹˜
        "avg_sessions_per_user": 3.76  # ì‹¤ì œ ê°’ (LLMë³´ë‹¤ 0.05 ë†’ìŒ)
    }
    
    execution_logs = [
        {"status": "success", "query_index": 0, "execution_time": 0.15, "tool": "mcp_everything"},
        {"status": "success", "query_index": 1, "execution_time": 0.12, "tool": "mcp_everything"},
        {"status": "success", "query_index": 0, "execution_time": 0.08, "tool": "mongodb_direct"},
        {"status": "success", "query_index": 1, "execution_time": 0.09, "tool": "mongodb_direct"}
    ]
    
    # í‰ê°€ ì‹¤í–‰
    metrics = quick_evaluate(
        analysis_query=analysis_query,
        mongodb_queries=mongodb_queries,
        calculation_results=llm_mcp_results,
        execution_logs=execution_logs,
        direct_mongodb_results=mongodb_direct_results,
        custom_thresholds={
            "semantic_error": 0.05,    # ë” ì—„ê²©í•œ ì„ê³„ê°’
            "execution_success": 0.95,
            "empty_result": 0.1,
            "accuracy": 0.85
        }
    )
    
    # ê²°ê³¼ ë¶„ì„
    print("\n" + "="*50)
    print("MCP INTEGRATION EXAMPLE RESULTS")
    print("="*50)
    
    print(f"\nì „ì²´ í‰ê°€: {'âœ… PASS' if metrics.overall_pass else 'âŒ FAIL'}")
    
    # ë¹„êµ í…Œì´ë¸” ì¶œë ¥
    if metrics.comparison_table is not None:
        print("\nğŸ“Š LLM+MCP vs MongoDB ì§ì ‘ ì‹¤í–‰ ë¹„êµ:")
        print("-" * 70)
        print(metrics.comparison_table.to_string(index=False))
        
        # ì¼ì¹˜/ë¶ˆì¼ì¹˜ ë¶„ì„
        matches = len(metrics.comparison_table[metrics.comparison_table['ì¼ì¹˜ ì—¬ë¶€'] == 'âœ… ì¼ì¹˜'])
        total = len(metrics.comparison_table)
        mismatch_rate = (total - matches) / total * 100 if total > 0 else 0
        
        print(f"\nğŸ“ˆ ì •í™•ë„ ë¶„ì„:")
        print(f"   - ì¼ì¹˜: {matches}/{total} ({(matches/total*100):.1f}%)")
        print(f"   - ë¶ˆì¼ì¹˜ìœ¨: {mismatch_rate:.1f}%")
        
        # ë¶ˆì¼ì¹˜ í•­ëª© ìƒì„¸ ë¶„ì„
        mismatches = metrics.comparison_table[metrics.comparison_table['ì¼ì¹˜ ì—¬ë¶€'] == 'âŒ ë¶ˆì¼ì¹˜']
        if not mismatches.empty:
            print(f"\nâš ï¸  ë¶ˆì¼ì¹˜ í•­ëª© ìƒì„¸:")
            for _, row in mismatches.iterrows():
                print(f"   - {row['ì§€í‘œ']}: LLM={row['LLM ê³„ì‚° ê²°ê³¼']} vs MongoDB={row['MongoDB ì§ì ‘ ì‹¤í–‰']} (ì°¨ì´: {row['ì°¨ì´']})")
    
    return metrics


def quality_assured_analysis_example():
    """í’ˆì§ˆ ë³´ì¥ ë¶„ì„ ì˜ˆì œ"""
    
    def mock_execute_llm_analysis(query):
        """LLM ë¶„ì„ ì‹¤í–‰ ëª¨ì˜ í•¨ìˆ˜"""
        class MockResult:
            def __init__(self):
                self.queries = [
                    "db.users.aggregate([{$group: {_id: '$status', count: {$sum: 1}}}])",
                    "db.users.find({active: true}).count()"
                ]
                self.data = {
                    "total_users": 1250,
                    "active_users": 980,
                    "activation_rate": 78.4
                }
        return MockResult()
    
    def mock_execute_mongodb_direct(queries):
        """MongoDB ì§ì ‘ ì‹¤í–‰ ëª¨ì˜ í•¨ìˆ˜"""
        return {
            "total_users": 1247,      # 3ëª… ì°¨ì´
            "active_users": 982,      # 2ëª… ì°¨ì´  
            "activation_rate": 78.7   # 0.3% ì°¨ì´
        }
    
    def quality_assured_analysis(query):
        """LLM ê³„ì‚°ê³¼ MongoDB ì§ì ‘ ì‹¤í–‰ì„ ë¹„êµí•˜ì—¬ í’ˆì§ˆ ë³´ì¥"""
        
        # 1. LLMìœ¼ë¡œ ë¶„ì„ ì‹¤í–‰
        llm_results = mock_execute_llm_analysis(query)
        
        # 2. MongoDBì—ì„œ ë™ì¼ ì¿¼ë¦¬ ì§ì ‘ ì‹¤í–‰
        mongodb_results = mock_execute_mongodb_direct(llm_results.queries)
        
        # 3. í’ˆì§ˆ í‰ê°€ (ë¹„êµ í¬í•¨)
        metrics = quick_evaluate(
            analysis_query=query,
            mongodb_queries=llm_results.queries,
            calculation_results=llm_results.data,
            direct_mongodb_results=mongodb_results,
            custom_thresholds={
                "semantic_error": 0.05,
                "execution_success": 0.95,
                "empty_result": 0.05,
                "accuracy": 0.90
            }
        )
        
        # 4. í’ˆì§ˆ ê²Œì´íŠ¸ ì²´í¬
        print(f"\nğŸ” í’ˆì§ˆ ê²€ì¦ ê²°ê³¼:")
        print(f"   - ì „ì²´ í‰ê°€: {'âœ… PASS' if metrics.overall_pass else 'âŒ FAIL'}")
        print(f"   - ì •í™•ë„: {metrics.accuracy_rate:.1%}")
        
        if not metrics.overall_pass:
            print("\nâŒ í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬:")
            print(f"   - ì˜ë¯¸ ì˜¤ë¥˜ìœ¨: {metrics.semantic_error_rate:.1%}")
            print(f"   - ì‹¤í–‰ ì„±ê³µë¥ : {metrics.execution_success_rate:.1%}")
            print(f"   - ë¬´ì‘ë‹µë¥ : {metrics.empty_result_rate:.1%}")
            print(f"   - ì •í™•ë„: {metrics.accuracy_rate:.1%}")
            
            if metrics.comparison_table is not None:
                print(f"\n   ğŸ“Š ë¶ˆì¼ì¹˜ í•­ëª©:")
                mismatches = metrics.comparison_table[
                    metrics.comparison_table['ì¼ì¹˜ ì—¬ë¶€'] == 'âŒ ë¶ˆì¼ì¹˜'
                ]
                for _, row in mismatches.iterrows():
                    print(f"      {row['ì§€í‘œ']}: ì°¨ì´ {row['ì°¨ì´']}")
            
            print("\nâš ï¸  ë¶„ì„ ê²°ê³¼ ì¬ê²€í†  í•„ìš”")
            return None
        
        print("âœ… í’ˆì§ˆ ê¸°ì¤€ í†µê³¼ - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ì…ë‹ˆë‹¤")
        return llm_results  # í’ˆì§ˆ ë³´ì¥ëœ ê²°ê³¼ë§Œ ë°˜í™˜
    
    # ì˜ˆì œ ì‹¤í–‰
    query = "ì‚¬ìš©ì í™œì„±í™” í˜„í™© ë¶„ì„"
    result = quality_assured_analysis(query)
    
    if result:
        print(f"\nğŸ“ˆ ìµœì¢… ë¶„ì„ ê²°ê³¼:")
        for key, value in result.data.items():
            print(f"   - {key}: {value}")


if __name__ == "__main__":
    print("=== ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ ===")
    basic_metrics = example_usage()
    
    print("\n\n=== MCP ì—°ë™ ì˜ˆì œ ===")
    mcp_metrics = example_with_mcp_integration()
    
    print("\n\n=== í’ˆì§ˆ ë³´ì¥ ë¶„ì„ ì˜ˆì œ ===")
    quality_assured_analysis_example()
